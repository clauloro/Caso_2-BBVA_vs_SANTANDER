{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3304ada3",
   "metadata": {},
   "source": [
    "## 3.2. Preprocesamiento de datos\n",
    "\n",
    "Para trabajar con los datos históricos de las acciones de **BBVA** y **Banco Santander**, se realizó un proceso exhaustivo de preprocesamiento con el fin de garantizar la calidad, consistencia y adecuación de los datos para su uso en modelos predictivos.\n",
    "\n",
    "### • Conversión de fechas\n",
    "La columna **Date**, que especifica la fecha de cada transacción, se transformó al formato de fecha estándar y se estableció como índice principal del conjunto de datos.  \n",
    "Esta operación facilita la manipulación y el análisis temporal de las series históricas de precios.\n",
    "\n",
    "### • Verificación de valores nulos\n",
    "Se verificó la existencia de valores faltantes en las columnas clave (**Open**, **High**, **Low**, **Close**, **Adj Close**, **Volume**).  \n",
    "Este paso fue crucial para evitar inconsistencias en el conjunto de datos.  \n",
    "En los archivos `BBVA_core_clean.csv` y `SAN_core_clean.csv` no se encontraron valores nulos, lo que permitió continuar directamente con los cálculos posteriores.\n",
    "\n",
    "### • Normalización de datos mediante *Min-Max Scaling*\n",
    "Para garantizar que todas las variables estuvieran en una escala comparable, se aplicó el método de escalado **Min-Max** a las columnas de precios (**Open**, **High**, **Low**, **Close**, **Adj Close**) y al volumen (**Volume**).  \n",
    "Este método es especialmente adecuado en contextos financieros, donde los datos presentan un rango acotado y la comparación entre variables requiere homogeneidad.\n",
    "\n",
    "**Fórmula:**\n",
    "\n",
    "$\n",
    "X' = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "$\n",
    "\n",
    "**Donde:**\n",
    "- \\( X' \\): Valor escalado entre 0 y 1  \n",
    "- \\( X \\): Valor original  \n",
    "- \\( X_{min} \\): Valor mínimo de la columna  \n",
    "- \\( X_{max} \\): Valor máximo de la columna  \n",
    "\n",
    "Este proceso garantiza que todas las variables tengan el mismo rango, mejorando la estabilidad numérica y el rendimiento de los algoritmos de aprendizaje automático.  \n",
    "Además, evita que las variables con rangos más amplios dominen sobre aquellas de menor magnitud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48b4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78bc83cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BBVA': WindowsPath('../data/interim/precios_limpios/BBVA_core_clean.csv'),\n",
       " 'SAN': WindowsPath('../data/interim/precios_limpios/SAN_core_clean.csv')}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Rutas y archivos\n",
    "RAW_DIR = Path(\"../data/interim/precios_limpios\")\n",
    "OUT_DIR = Path(\"../data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "files = {\n",
    "    \"BBVA\": RAW_DIR / \"BBVA_core_clean.csv\",\n",
    "    \"SAN\" : RAW_DIR / \"SAN_core_clean.csv\",\n",
    "}\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4357384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Utilidades\n",
    "KEY_COLS = [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\"]\n",
    "PRESERVE_COLS = [\"Dividends\",\"Stock Splits\",\"Dividends_bin\"]  # No se escalan\n",
    "ALL_EXPECTED = [\"Date\",\"Adj Close\",\"Close\",\"Dividends\",\"High\",\"Low\",\"Open\",\"Stock Splits\",\"Volume\",\"Dividends_bin\"]\n",
    "\n",
    "def ensure_date_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Comprueba/corrige Date como datetime y lo pone como índice.\"\"\"\n",
    "    if \"Date\" not in df.columns:\n",
    "        raise ValueError(\"No se encontró la columna 'Date' en el CSV.\")\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[\"Date\"]):\n",
    "        # intenta convertir\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
    "    # seguridad: elimina filas sin fecha válida\n",
    "    df = df.dropna(subset=[\"Date\"]).copy()\n",
    "    df = df.sort_values(\"Date\")\n",
    "    df = df.set_index(\"Date\")\n",
    "    return df\n",
    "\n",
    "def minmax_scale_df(df: pd.DataFrame, cols: list[str]) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"Aplica Min-Max a las columnas indicadas y devuelve df escalado + params.\"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = df.copy()\n",
    "    scaler.fit(df_scaled[cols])\n",
    "    df_scaled[cols] = scaler.transform(df_scaled[cols])\n",
    "    # guardar parámetros para reproducibilidad\n",
    "    params = {c: {\"min\": float(scaler.data_min_[i]), \"max\": float(scaler.data_max_[i])} \n",
    "              for i, c in enumerate(cols)}\n",
    "    return df_scaled, params\n",
    "\n",
    "def quick_null_report(df: pd.DataFrame, cols: list[str]) -> pd.Series:\n",
    "    \"\"\"Conteo de nulos en columnas clave.\"\"\"\n",
    "    return df[cols].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6204abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBVA -> filas: 6,634 | rango fechas: 2000-01-03 → 2025-10-30\n",
      "SAN -> filas: 6,634 | rango fechas: 2000-01-03 → 2025-10-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lopec\\AppData\\Local\\Temp\\ipykernel_21996\\2821945429.py:12: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
      "C:\\Users\\lopec\\AppData\\Local\\Temp\\ipykernel_21996\\2821945429.py:12: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", utc=False, infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "# 3) Carga, validación de columnas y conversión de fechas\n",
    "data_raw = {}\n",
    "for name, path in files.items():\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Validación de columnas esperadas (permite columnas extra)\n",
    "    missing = [c for c in ALL_EXPECTED if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{name}: faltan columnas {missing}\")\n",
    "    \n",
    "    # Asegurar Date como índice datetime\n",
    "    df = ensure_date_index(df)\n",
    "\n",
    "    data_raw[name] = df\n",
    "    print(f\"{name} -> filas: {len(df):,} | rango fechas: {df.index.min().date()} → {df.index.max().date()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2f549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nulls en BBVA (solo columnas clave):\n",
      "Open         0\n",
      "High         0\n",
      "Low          0\n",
      "Close        0\n",
      "Adj Close    0\n",
      "Volume       0\n",
      "dtype: int64\n",
      "\n",
      "Nulls en SAN (solo columnas clave):\n",
      "Open         0\n",
      "High         0\n",
      "Low          0\n",
      "Close        0\n",
      "Adj Close    0\n",
      "Volume       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 4) Verificación de valores nulos en columnas clave\n",
    "for name, df in data_raw.items():\n",
    "    nulls = quick_null_report(df, KEY_COLS)\n",
    "    print(f\"\\nNulls en {name} (solo columnas clave):\\n{nulls}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1287cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBVA: guardado escalado -> ..\\data\\processed\\BBVA_core_scaled.csv\n",
      "SAN: guardado escalado -> ..\\data\\processed\\SAN_core_scaled.csv\n"
     ]
    }
   ],
   "source": [
    "# 5) Min-Max Scaling en precios y volumen\n",
    "scaled = {}\n",
    "scaler_params = {}\n",
    "\n",
    "for name, df in data_raw.items():\n",
    "    # Copia de trabajo: nos quedamos con todas las columnas\n",
    "    df_work = df.copy()\n",
    "\n",
    "    # Escalar solo columnas KEY_COLS; el resto se preserva tal cual\n",
    "    df_scaled, params = minmax_scale_df(df_work, KEY_COLS)\n",
    "    scaled[name] = df_scaled\n",
    "    scaler_params[name] = params\n",
    "\n",
    "    # Guardado CSV escalado\n",
    "    out_csv = OUT_DIR / f\"{name}_core_scaled.csv\"\n",
    "    df_scaled.to_csv(out_csv, index=True)\n",
    "    print(f\"{name}: guardado escalado -> {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e8e6dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros de escalado guardados en: ..\\data\\processed\\scalers_minmax_params.json\n"
     ]
    }
   ],
   "source": [
    "# 6) Guardar parámetros de escalado (min y max por columna) en JSON para reproducibilidad\n",
    "params_path = OUT_DIR / \"scalers_minmax_params.json\"\n",
    "with open(params_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scaler_params, f, indent=2, ensure_ascii=False)\n",
    "print(\"Parámetros de escalado guardados en:\", params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c275e1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BBVA - rangos tras Min-Max (esperado ~0..1):\n",
      "     Open  High  Low  Close  Adj Close  Volume\n",
      "min   0.0   0.0  0.0    0.0        0.0     0.0\n",
      "max   1.0   1.0  1.0    1.0        1.0     1.0\n",
      "\n",
      "SAN - rangos tras Min-Max (esperado ~0..1):\n",
      "     Open  High  Low  Close  Adj Close  Volume\n",
      "min   0.0   0.0  0.0    0.0        0.0     0.0\n",
      "max   1.0   1.0  1.0    1.0        1.0     1.0\n"
     ]
    }
   ],
   "source": [
    "# 7) (Opcional) Comprobación rápida del resultado\n",
    "for name, df in scaled.items():\n",
    "    desc = df[KEY_COLS].describe().loc[[\"min\",\"max\"]]\n",
    "    print(f\"\\n{name} - rangos tras Min-Max (esperado ~0..1):\\n{desc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Caso2 (env)",
   "language": "python",
   "name": "caso2-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
